{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnujGogate/DE/blob/main/Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6W_fiLZzZfB"
      },
      "source": [
        "##Spark Boiler Plate Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "t7Q4zw0Nza5N",
        "outputId": "239655da-432d-4693-8d85-9770eed7aa66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fef872bd960>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f0663b036299:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, BooleanType, FloatType, MapType, ArrayType, DateType, LongType\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import *\n",
        "from pyspark.sql import *\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiJ9YS_T3hzU"
      },
      "source": [
        "##Spark Essentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0EbHFZ18RHj"
      },
      "source": [
        "###Word Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLIL2XNu3hzV"
      },
      "outputs": [],
      "source": [
        "text = \"Hello, World How are you. I'm fine, but tell me how are you too. HeLLo,\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy2NOYWf4zDw",
        "outputId": "ea7bdc3e-e296-4241-f51f-dc2075458639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('', 28), (\"'\", 1), (',', 3), ('.', 2), ('a', 2), ('b', 1), ('d', 1), ('e', 7), ('f', 1), ('h', 4), ('i', 2), ('l', 7), ('m', 2), ('n', 1), ('o', 9), ('r', 3), ('t', 3), ('u', 3), ('w', 3), ('y', 2)]\n"
          ]
        }
      ],
      "source": [
        "rdd_1 = spark.sparkContext.parallelize(text).flatMap(lambda x : x.lower().split(\" \"),1)\n",
        "\n",
        "rdd_2 = rdd_1.map(lambda x : (x,1))\n",
        "\n",
        "rdd_3 = rdd_2.reduceByKey(lambda x,y : x+y).sortByKey().collect()\n",
        "\n",
        "print(rdd_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LHufnI79vbO"
      },
      "source": [
        "###Handling NULL's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGoxn_D59vbV"
      },
      "outputs": [],
      "source": [
        "data = [(1, 'Anuj', None), (2, 'Rahul', 102), (3, 'John', 101)]\n",
        "\n",
        "df = spark.createDataFrame(data, schema = ('id', 'name', 'dept'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "YfK3tNxp9vbV",
        "outputId": "46c3bf7d-4f74-436a-acd1-0a4e78206d43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----+-------+\n",
            "| id| name|   dept|\n",
            "+---+-----+-------+\n",
            "|  1| Anuj|Unknown|\n",
            "|  2|Rahul|    102|\n",
            "|  3| John|    101|\n",
            "+---+-----+-------+\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>name</th><th>isNull?</th></tr>\n",
              "<tr><td>Anuj</td><td>true</td></tr>\n",
              "<tr><td>Rahul</td><td>false</td></tr>\n",
              "<tr><td>John</td><td>false</td></tr>\n",
              "</table>\n"
            ],
            "text/plain": [
              "+-----+-------+\n",
              "| name|isNull?|\n",
              "+-----+-------+\n",
              "| Anuj|   true|\n",
              "|Rahul|  false|\n",
              "| John|  false|\n",
              "+-----+-------+"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, coalesce, avg\n",
        "\n",
        "#df1 = df.fillna(\"unknown\",subset=[\"dept\"])\n",
        "\n",
        "df2 = df.withColumn(\"dept\",when(col(\"dept\").isNull(),\"Unknown\").otherwise(col(\"dept\")))\n",
        "\n",
        "df3 = df.withColumn(\"dept\",coalesce(col(\"dept\"),lit(\"Unknown\")))\n",
        "\n",
        "df3.show()\n",
        "\n",
        "df.select(df.name, isnull(df.dept).alias('isNull?'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1l28s_0C-hM"
      },
      "source": [
        "###Dropping NULL's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t2l2LyzC-hT"
      },
      "outputs": [],
      "source": [
        "data = [('Alice', 80, 10), ('Bob', None, 5), ('Tom', 50, 50), (None, None, None), ('Robert', 30, 35)] ; schema = 'name string, age int, height int'\n",
        "\n",
        "df = spark.createDataFrame(data, schema = schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9QuP08iC-hU",
        "outputId": "f31c25a2-56b1-4c8e-8983-5941d8d85457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+---+------+\n",
            "|  name|age|height|\n",
            "+------+---+------+\n",
            "| Alice| 80|    10|\n",
            "|   Bob| 53|     5|\n",
            "|   Tom| 50|    50|\n",
            "|Robert| 30|    35|\n",
            "+------+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "avg_age = df1.select(floor(avg(\"age\"))).collect()[0][0]\n",
        "\n",
        "df1 = df.fillna(avg_age,subset = [\"age\"])\n",
        "\n",
        "df2 = df1.dropna()\n",
        "\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7l8sCHZF8iC"
      },
      "source": [
        "###Create Dataframe with range 100 records and filter records from 40 to 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLeSG9dIF8iC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a9c3911-8916-47cb-9611-fda02b95c46e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|values|\n",
            "+------+\n",
            "|     1|\n",
            "|     2|\n",
            "|     3|\n",
            "|     4|\n",
            "|     5|\n",
            "|     6|\n",
            "|     7|\n",
            "|     8|\n",
            "|     9|\n",
            "|    10|\n",
            "+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.createDataFrame([(x,) for x in range(1,11)], schema = \"values int\")\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fjz3Q_noF8iC",
        "outputId": "f13f8d0e-1930-44fe-f37a-2fc9b0b83ab2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+\n",
            "|values|\n",
            "+------+\n",
            "|     4|\n",
            "|     5|\n",
            "|     6|\n",
            "|     7|\n",
            "|     8|\n",
            "+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1 = df.filter((col(\"values\") >= 4) & (col(\"values\") <= 8))\n",
        "\n",
        "df2 = df.where(col(\"values\").between(4,8))\n",
        "\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll2HeJdtMxDQ"
      },
      "source": [
        "###Merge Two Array Dataframe\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5BQpgJwMxDQ"
      },
      "outputs": [],
      "source": [
        "data_1 = [(1, 'Anuj', 103), (2, 'William', 102), (3, 'John', 101)]\n",
        "\n",
        "data_2 = [(4, 'Sam', 105), (5, 'Rocky', 106), (6, 'Steve', 107)]\n",
        "\n",
        "df_1 = spark.createDataFrame(data_1, schema = ('id', 'name', 'dept'))\n",
        "\n",
        "df_2 = spark.createDataFrame(data_2, schema = ('id', 'name', 'dept'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1_xs9M0MxDR",
        "outputId": "9a26842d-d0f2-4b38-a090-29cdae20f63e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-------+----+--------+\n",
            "| id|   name|dept|location|\n",
            "+---+-------+----+--------+\n",
            "|  1|   Anuj| 103|    null|\n",
            "|  2|William| 102|    null|\n",
            "|  3|   John| 101|    null|\n",
            "|  4|    Sam| 105|    null|\n",
            "|  5|  Rocky| 106|    null|\n",
            "|  6|  Steve| 107|    null|\n",
            "+---+-------+----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1 = df_1.withColumn(\"location\",lit(None))\n",
        "\n",
        "df2 = df1.unionByName(df_2,allowMissingColumns=True)\n",
        "\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MltrPMVjRZ2K",
        "outputId": "6d612f9d-65e9-4b6b-bde7-78573af8737b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------+--------------+\n",
            "|symbol|     firstname|      lastname|\n",
            "+------+--------------+--------------+\n",
            "|     A|  [Anuj, John]|[Gogate, Wick]|\n",
            "|     B|[Bruce, Clark]| [Wayne, Kent]|\n",
            "+------+--------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [(\"A\", [\"Anuj\", \"John\"], [\"Gogate\", \"Wick\"]), (\"B\", [\"Bruce\", \"Clark\"], [\"Wayne\", \"Kent\"])]\n",
        "columns = [\"symbol\", \"firstname\", \"lastname\"]\n",
        "\n",
        "# Create a DataFrame from the input data\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKDbWaZSRdzD",
        "outputId": "ab724848-9506-430e-bb84-afb085d80ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------+--------------+--------------+\n",
            "|symbol|     firstname|      lastname|     full_name|\n",
            "+------+--------------+--------------+--------------+\n",
            "|     A|  [Anuj, John]|[Gogate, Wick]|{Anuj, Gogate}|\n",
            "|     A|  [Anuj, John]|[Gogate, Wick]|  {John, Wick}|\n",
            "|     B|[Bruce, Clark]| [Wayne, Kent]|{Bruce, Wayne}|\n",
            "|     B|[Bruce, Clark]| [Wayne, Kent]| {Clark, Kent}|\n",
            "+------+--------------+--------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1 = df.select(df.symbol,df.firstname,df.lastname,explode(arrays_zip(df.firstname,df.lastname)).alias(\"full_name\"))\n",
        "\n",
        "df1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFG5ZcwoWK5l"
      },
      "source": [
        "###Extract Name from email id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaWEqQlBWK5m",
        "outputId": "9c864d25-be59-467b-94d4-55778dd9b381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------+--------+\n",
            "|email                   |empId   |\n",
            "+------------------------+--------+\n",
            "|anuj.gogate@teradata.com|AG255120|\n",
            "|john.bravo@teradata.com |FS250040|\n",
            "+------------------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = [{\"empId\" : \"AG255120\", \"email\": \"anuj.gogate@teradata.com\"},\n",
        "        {\"empId\" : \"FS250040\", \"email\": \"john.bravo@teradata.com\"}]\n",
        "\n",
        "df = spark.createDataFrame(info)\n",
        "\n",
        "df.show(truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, col, initcap, concat_ws\n",
        "\n",
        "# Split the email column by \"@\" and take the first part\n",
        "df1 = df.withColumn(\"name_split\", split(col(\"email\"), \"@\")[0])\n",
        "\n",
        "# Split the first part by \".\" to separate the first and last names\n",
        "df2 = df1.withColumn(\"name_parts\", split(col(\"name_split\"), \"\\\\.\"))\n",
        "\n",
        "# Capitalize the first and last names, and concatenate them with a space in between\n",
        "df3 = df2.withColumn(\"Full_name\", concat_ws(\" \", initcap(col(\"name_parts\")[0]), initcap(col(\"name_parts\")[1])))\n",
        "\n",
        "# Select only the required columns\n",
        "final_df = df3.select(\"email\", \"empId\", \"Full_name\")\n",
        "\n",
        "df2.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg-gq59HfyWr",
        "outputId": "49bbe9a7-0766-4af4-978d-1a11c449b1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------+--------+-----------+--------------+\n",
            "|email                   |empId   |name_split |name_parts    |\n",
            "+------------------------+--------+-----------+--------------+\n",
            "|anuj.gogate@teradata.com|AG255120|anuj.gogate|[anuj, gogate]|\n",
            "|john.bravo@teradata.com |FS250040|john.bravo |[john, bravo] |\n",
            "+------------------------+--------+-----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEVLbaeNkmij",
        "outputId": "fa60ec6f-e7b9-41c7-ccdc-2f1c3c751a61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------+-----------+\n",
            "|               email|   empId|  Full_name|\n",
            "+--------------------+--------+-----------+\n",
            "|anuj.gogate@terad...|AG255120|Anuj Gogate|\n",
            "|john.bravo@terada...|FS250040| John Bravo|\n",
            "+--------------------+--------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "@udf (returnType = StringType())\n",
        "def get_names(p_str):\n",
        "  str1 = p_str.split(\"@\")[0].split(\".\")\n",
        "  return str1[0][0].upper() + str1[0][1:] + \" \" + str1[1][0].upper() + str1[1][1:]\n",
        "\n",
        "df1 = df.select(\"*\",get_names(\"email\").alias(\"Full_name\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao9XEO1dWK5m",
        "outputId": "4c0b49b9-8f12-4f07-c19b-a44f04aeb544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------+--------+-----------+\n",
            "|email                   |empId   |full_name  |\n",
            "+------------------------+--------+-----------+\n",
            "|anuj.gogate@teradata.com|AG255120|Anuj Gogate|\n",
            "|john.bravo@teradata.com |FS250040|John Bravo |\n",
            "+------------------------+--------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1 = df.withColumn(\"full_name\", \\\n",
        "      concat_ws(\" \", \\\n",
        "      (initcap(split(col(\"email\"),\"[@.]\")[0])), \\\n",
        "      (initcap(split(col(\"email\"),\"[@.]\")[1])) \\\n",
        "      ))\n",
        "\n",
        "df1.show(truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRD7m0RN_rCT"
      },
      "source": [
        "###Renaming columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('a', 100, 'HR'), ('b', 200, 'Manager'), ('c', 300, 'Manager'), ('d', 400, 'HR'), ('e', 500, 'HR'), ('f', 600, 'Manager')]\n",
        "\n",
        "schema = [\"name\", \"salary\", \"dept\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema = schema)\n",
        "\n",
        "df.orderBy('dept')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "q45XKCxD_0l3",
        "outputId": "a684a904-353d-4cbe-fbfb-ed9df0dc606d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----+------+-------+\n",
              "|name|salary|   dept|\n",
              "+----+------+-------+\n",
              "|   e|   500|     HR|\n",
              "|   a|   100|     HR|\n",
              "|   d|   400|     HR|\n",
              "|   f|   600|Manager|\n",
              "|   b|   200|Manager|\n",
              "|   c|   300|Manager|\n",
              "+----+------+-------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>name</th><th>salary</th><th>dept</th></tr>\n",
              "<tr><td>e</td><td>500</td><td>HR</td></tr>\n",
              "<tr><td>a</td><td>100</td><td>HR</td></tr>\n",
              "<tr><td>d</td><td>400</td><td>HR</td></tr>\n",
              "<tr><td>f</td><td>600</td><td>Manager</td></tr>\n",
              "<tr><td>b</td><td>200</td><td>Manager</td></tr>\n",
              "<tr><td>c</td><td>300</td><td>Manager</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_col = \"new_\"\n",
        "\n",
        "df1 = df.select([df[x].alias(f\"{new_col}{x}\")  for x in df.columns])\n",
        "\n",
        "df1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "1geoeFHHACNe",
        "outputId": "af698f73-992a-4e2e-b7ea-25c78ab840fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+--------+----------+--------+\n",
              "|new_name|new_salary|new_dept|\n",
              "+--------+----------+--------+\n",
              "|       a|       100|      HR|\n",
              "|       b|       200| Manager|\n",
              "|       c|       300| Manager|\n",
              "|       d|       400|      HR|\n",
              "|       e|       500|      HR|\n",
              "|       f|       600| Manager|\n",
              "+--------+----------+--------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>new_name</th><th>new_salary</th><th>new_dept</th></tr>\n",
              "<tr><td>a</td><td>100</td><td>HR</td></tr>\n",
              "<tr><td>b</td><td>200</td><td>Manager</td></tr>\n",
              "<tr><td>c</td><td>300</td><td>Manager</td></tr>\n",
              "<tr><td>d</td><td>400</td><td>HR</td></tr>\n",
              "<tr><td>e</td><td>500</td><td>HR</td></tr>\n",
              "<tr><td>f</td><td>600</td><td>Manager</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q8hU_F6HexK"
      },
      "source": [
        "###Spliting based on Pipe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"a|b\"),\n",
        "    (2, \"a|e|d\"),\n",
        "    (3, \"f\")\n",
        "]\n",
        "\n",
        "schema = [\"c1\", \"c2\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e6de3ad-d871-4645-8432-8a5ea4745061",
        "id": "njXTwcivHexL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|c1 |c2   |\n",
            "+---+-----+\n",
            "|1  |a|b  |\n",
            "|2  |a|e|d|\n",
            "|3  |f    |\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.withColumn(\"c2\",explode(split(col(\"c2\"),\"\\|\"))).show()\n",
        "\n",
        "\"\"\"\n",
        "Key Difference:\n",
        "\"|\": Treated as a regular expression (logical OR operator).\n",
        "\"\\\\|\": Escaped pipe, treated as a literal character.\n",
        "\n",
        "Best Practice:\n",
        "When working with characters that have special meanings in regular expressions, such as |, ., *, +, etc., always escape them using a backslash (\\) if you want to treat them literally.\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "d466536c-319b-4327-dc77-44b9c58a49d1",
        "id": "55BTX8CBHexL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "| c1| c2|\n",
            "+---+---+\n",
            "|  1|  a|\n",
            "|  1|  b|\n",
            "|  2|  a|\n",
            "|  2|  e|\n",
            "|  2|  d|\n",
            "|  3|  f|\n",
            "+---+---+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nKey Difference:\\n\"|\": Treated as a regular expression (logical OR operator).\\n\"\\\\|\": Escaped pipe, treated as a literal character.\\n\\nBest Practice:\\nWhen working with characters that have special meanings in regular expressions, such as |, ., *, +, etc., always escape them using a backslash (\\\\) if you want to treat them literally.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DM1vTfTxWm8R"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG6lmkLVWnWG"
      },
      "source": [
        "###Filter based on comments in a column"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"date_searched\", StringType(), True),\n",
        "    StructField(\"filter_room_types\", StringType(), True)\n",
        "])\n",
        "# Data\n",
        "data = [\n",
        "    (1, \"2022-01-01\", \"entire home,couple room,private room\"),\n",
        "    (2, \"2022-01-02\", \"entire home,shared room\"),\n",
        "    (3, \"2022-01-02\", \"private room\"),\n",
        "    (4, \"2022-01-03\", \"private room\"),\n",
        "    (5, \"2022-01-04\", \"entire home,private room,shared room,couple room\"),\n",
        "    (6, \"2022-01-05\", \"entire home,shared room\"),\n",
        "    (7, \"2022-01-06\", \"private room,couple room,private room\"),\n",
        "    (8, \"2022-01-07\", \"entire home,shared room\"),\n",
        "    (9, \"2022-01-08\", \"private room,shared room\"),\n",
        "    (10, \"2022-01-09\", \"entire home\")\n",
        "]\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d512a4e4-46fa-4beb-8e6f-079cf0869998",
        "id": "jIdH2YkLWnWG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+------------------------------------------------+\n",
            "|user_id|date_searched|filter_room_types                               |\n",
            "+-------+-------------+------------------------------------------------+\n",
            "|1      |2022-01-01   |entire home,couple room,private room            |\n",
            "|2      |2022-01-02   |entire home,shared room                         |\n",
            "|3      |2022-01-02   |private room                                    |\n",
            "|4      |2022-01-03   |private room                                    |\n",
            "|5      |2022-01-04   |entire home,private room,shared room,couple room|\n",
            "|6      |2022-01-05   |entire home,shared room                         |\n",
            "|7      |2022-01-06   |private room,couple room,private room           |\n",
            "|8      |2022-01-07   |entire home,shared room                         |\n",
            "|9      |2022-01-08   |private room,shared room                        |\n",
            "|10     |2022-01-09   |entire home                                     |\n",
            "+-------+-------------+------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.withColumn(\"room_types\",explode(split(\"filter_room_types\",\",\"))) \\\n",
        "        .filter(col(\"room_types\").like(\"%room\"))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c772bd7-22f5-4680-93d2-9ed9691c8dd9",
        "id": "QPDbofuOWnWG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------+--------------------+------------+\n",
            "|user_id|date_searched|   filter_room_types|  room_types|\n",
            "+-------+-------------+--------------------+------------+\n",
            "|      1|   2022-01-01|entire home,coupl...| couple room|\n",
            "|      1|   2022-01-01|entire home,coupl...|private room|\n",
            "|      2|   2022-01-02|entire home,share...| shared room|\n",
            "|      3|   2022-01-02|        private room|private room|\n",
            "|      4|   2022-01-03|        private room|private room|\n",
            "|      5|   2022-01-04|entire home,priva...|private room|\n",
            "|      5|   2022-01-04|entire home,priva...| shared room|\n",
            "|      5|   2022-01-04|entire home,priva...| couple room|\n",
            "|      6|   2022-01-05|entire home,share...| shared room|\n",
            "|      7|   2022-01-06|private room,coup...|private room|\n",
            "|      7|   2022-01-06|private room,coup...| couple room|\n",
            "|      7|   2022-01-06|private room,coup...|private room|\n",
            "|      8|   2022-01-07|entire home,share...| shared room|\n",
            "|      9|   2022-01-08|private room,shar...|private room|\n",
            "|      9|   2022-01-08|private room,shar...| shared room|\n",
            "+-------+-------------+--------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr_nylPmfHvw"
      },
      "source": [
        "###Finding Average Salary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add a flag which indicates if the salary of employee is above or below average\n",
        "data = [('a', 100, 'HR'), ('b', 200, 'Manager'), ('c', 300, 'Manager'), ('d', 400, 'HR'), ('e', 500, 'HR'), ('f', 600, 'Manager')]\n",
        "\n",
        "schema = [\"name\", \"salary\", \"dept\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema = schema)\n",
        "\n",
        "df.orderBy('dept')"
      ],
      "metadata": {
        "id": "Nb-5QnLafUaV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "ef440b78-6cfc-4018-9ae8-c79b312864cd"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----+------+-------+\n",
              "|name|salary|   dept|\n",
              "+----+------+-------+\n",
              "|   a|   100|     HR|\n",
              "|   e|   500|     HR|\n",
              "|   d|   400|     HR|\n",
              "|   b|   200|Manager|\n",
              "|   f|   600|Manager|\n",
              "|   c|   300|Manager|\n",
              "+----+------+-------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>name</th><th>salary</th><th>dept</th></tr>\n",
              "<tr><td>a</td><td>100</td><td>HR</td></tr>\n",
              "<tr><td>e</td><td>500</td><td>HR</td></tr>\n",
              "<tr><td>d</td><td>400</td><td>HR</td></tr>\n",
              "<tr><td>b</td><td>200</td><td>Manager</td></tr>\n",
              "<tr><td>f</td><td>600</td><td>Manager</td></tr>\n",
              "<tr><td>c</td><td>300</td><td>Manager</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_salary = df.agg(floor(avg(\"salary\"))).collect()[0][0]\n",
        "\n",
        "df1 = df.withColumn(\"salary_chk\",when(col(\"salary\") > avg_salary,\"Salary is greater than average\").otherwise(\"Salary is under Average\")).show(7,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_tNLJG7hCka",
        "outputId": "5e90370c-6468-4137-fabc-19b980cad1a8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+-------+------------------------------+\n",
            "|name|salary|dept   |salary_chk                    |\n",
            "+----+------+-------+------------------------------+\n",
            "|a   |100   |HR     |Salary is under Average       |\n",
            "|b   |200   |Manager|Salary is under Average       |\n",
            "|c   |300   |Manager|Salary is under Average       |\n",
            "|d   |400   |HR     |Salary is greater than average|\n",
            "|e   |500   |HR     |Salary is greater than average|\n",
            "|f   |600   |Manager|Salary is greater than average|\n",
            "+----+------+-------+------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjM5FXGri2xR"
      },
      "source": [
        "##Tiger Analytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psmvJdm2qyu0"
      },
      "source": [
        "###Count of each Join"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e82416-b00a-454b-dbc6-94497d362c90",
        "id": "_F_PZhiSqyu0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----------+\n",
            "|cust_id|origin|destination|\n",
            "+-------+------+-----------+\n",
            "|      1| Delhi|  Mangalore|\n",
            "|      2|Mumbai|    Chennai|\n",
            "+-------+------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define data for Table A and Table B\n",
        "data_A = [(1,), (0,), (0,), (None,), (1,), (None,), (0,)]\n",
        "data_B = [(1,), (0,), (1,), (None,), (1,), (0,)]\n",
        "\n",
        "# Define schemas for Table A and Table B\n",
        "columns_A = [\"A\"]\n",
        "columns_B = [\"B\"]\n",
        "\n",
        "# Create DataFrames for Table A and Table B\n",
        "df_A = spark.createDataFrame(data_A, columns_A)\n",
        "df_B = spark.createDataFrame(data_B, columns_B)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the various joins and unions\n",
        "\n",
        "# 1. Inner Join\n",
        "inner_join_df = df_A.join(df_B, df_A.A == df_B.B, \"inner\")\n",
        "print(\"Inner Join Result:\")\n",
        "inner_join_df.show()\n",
        "\n",
        "# 2. Left Join\n",
        "left_join_df = df_A.join(df_B, df_A.A == df_B.B, \"left\")\n",
        "print(\"Left Join Result:\")\n",
        "left_join_df.show()\n",
        "\n",
        "# 3. Right Join\n",
        "right_join_df = df_A.join(df_B, df_A.A == df_B.B, \"right\")\n",
        "print(\"Right Join Result:\")\n",
        "right_join_df.show()\n",
        "\n",
        "# 4. Full Outer Join\n",
        "outer_join_df = df_A.join(df_B, df_A.A == df_B.B, \"outer\")\n",
        "print(\"Full Outer Join Result:\")\n",
        "outer_join_df.show()\n",
        "\n",
        "# 5. Union (removing duplicates)\n",
        "union_df = df_A.union(df_B).distinct()\n",
        "print(\"Union Result:\")\n",
        "union_df.show()\n",
        "\n",
        "# 6. Union All (including duplicates)\n",
        "union_all_df = df_A.union(df_B)\n",
        "print(\"Union All Result:\")\n",
        "union_all_df.show()\n"
      ],
      "metadata": {
        "id": "OYfQtrepqyu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0cmKiQjjA6L"
      },
      "source": [
        "###finding the first origin and final destination for each customer based on a sequence of flights taken."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e82416-b00a-454b-dbc6-94497d362c90",
        "id": "VxElAHdTjA6M"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----------+\n",
            "|cust_id|origin|destination|\n",
            "+-------+------+-----------+\n",
            "|      1| Delhi|  Mangalore|\n",
            "|      2|Mumbai|    Chennai|\n",
            "+-------+------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "flights_data = [(1,'Flight1' , 'Delhi' , 'Hyderabad'),\n",
        " (1,'Flight2' , 'Hyderabad' , 'Kochi'),\n",
        " (1,'Flight3' , 'Kochi' , 'Mangalore'),\n",
        " (2,'Flight1' , 'Mumbai' , 'Ayodhya'),\n",
        " (2,'Flight2' , 'Ayodhya' , 'Chennai')\n",
        " ]\n",
        "\n",
        "my_schema = \"cust_id int, flight_id string , origin string , destination string\"\n",
        "\n",
        "df = spark.createDataFrame(data = flights_data, schema = my_schema)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#spec = Window.partitionBy(\"cust_id\").orderBy(\"flight_id\")\n",
        "\n",
        "df1 = df.groupBy(\"cust_id\").agg(first(\"origin\").alias(\"origin\"),last(\"destination\").alias(\"destination\"))\n",
        "\n",
        "df1.show()"
      ],
      "metadata": {
        "id": "YtVltk87jA6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJbUObontn6I"
      },
      "source": [
        "###Calculate the TotalQty, TotalSales, and MoMProfit for each mont"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJK1wIFOtn6J"
      },
      "outputs": [],
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"SalesData\").getOrCreate()\n",
        "\n",
        "# Define the raw data\n",
        "data = [\n",
        "    (\"Ramesh\", 20, \"Jan\", 50000),\n",
        "    (\"Deep\", 25, \"Jan\", 30000),\n",
        "    (\"Suresh\", 22, \"Feb\", 50000),\n",
        "    (\"Ram\", 28, \"Feb\", 20000),\n",
        "    (\"Pradeep\", 22, \"Feb\", 20000),\n",
        "    (\"Deep\", 25, \"Mar\", 30000),\n",
        "    (\"Suresh\", 22, \"Mar\", 50000),\n",
        "    (\"Ram\", 28, \"Mar\", 20000),\n",
        "    (\"Pradeep\", 22, \"Mar\", 20000)\n",
        "]\n",
        "\n",
        "# Define column names\n",
        "columns = [\"Name\", \"Qty\", \"Month\", \"Sales\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Aggregate data by Month to get TotalQty and TotalSales\n",
        "monthly_df = df.groupBy(\"Month\") \\\n",
        "               .agg(\n",
        "                   sum(\"Qty\").alias(\"TotalQty\"),\n",
        "                   sum(\"Sales\").alias(\"TotalSales\")\n",
        "               )\n",
        "\n",
        "# Step 2: Define a window to calculate the previous month's sales for MoMProfit calculation\n",
        "windowSpec = Window.orderBy(\"Month\")\n",
        "\n",
        "# Step 3: Calculate MoMProfit\n",
        "result_df = monthly_df.withColumn(\"PrevMonthSales\", lag(\"TotalSales\").over(windowSpec)) \\\n",
        "                      .withColumn(\"MoMProfit\", col(\"TotalSales\") - col(\"PrevMonthSales\")) \\\n",
        "                      .select(\"Month\", \"TotalQty\", \"TotalSales\", \"MoMProfit\")\n",
        "\n",
        "# Show the result\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yik2jMwztn6J",
        "outputId": "dd3bf182-9578-4196-9628-a64b6ae90d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+----------+--------------+\n",
            "|Month|TotalQty|TotalSales|PrevMonthSales|\n",
            "+-----+--------+----------+--------------+\n",
            "|  Feb|      72|     90000|          null|\n",
            "|  Jan|      45|     80000|         90000|\n",
            "|  Mar|      97|    120000|         80000|\n",
            "+-----+--------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dlj5OVKDunB1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "I6W_fiLZzZfB",
        "B0EbHFZ18RHj",
        "5LHufnI79vbO",
        "Q1l28s_0C-hM",
        "m7l8sCHZF8iC",
        "Ll2HeJdtMxDQ",
        "zFG5ZcwoWK5l",
        "MRD7m0RN_rCT",
        "0Q8hU_F6HexK",
        "UG6lmkLVWnWG",
        "hr_nylPmfHvw",
        "PjM5FXGri2xR",
        "psmvJdm2qyu0",
        "K0cmKiQjjA6L",
        "dJbUObontn6I"
      ],
      "authorship_tag": "ABX9TyM2/73zbj7W3HhFhxu0zZBf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}