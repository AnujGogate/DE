Inteview is where we need to kill the time
https://www.youtube.com/watch?v=jQhc1NwdOOg&list=WL&index=1&t=718s&ab_channel=AzurelibAcademy

https://www.linkedin.com/posts/shubhamwadekar_data-warehousing-interview-questions-%F0%9D%97%A3%F0%9D%97%9B%F0%9D%97%94%F0%9D%97%A6%F0%9D%97%98-activity-7247250823871705095-J9WD

Self-Introduction
==================
My name is Anuj Gogate. I am from Pune. Currently working as Data Engineer in Teradata having total 4+ years of IT experience. I am proficient in ACID data engineer using ADF, Azure Databricks. I have command in writing Pyspark, Python and SQL. This is brief of myself.

My data engineer roles is to transform the raw data into suitaable for DA, DS, ML purpose. We are using Azure in our project 

Retail-hub
===========

End-to-End project structure -
1] Its a Retail Domain project. This is a project of Data lake house project where we follow medallion architecture of bronze, silver and gold layers.
2] The Injection of the data happens from the upstream system we have a data source like SQL DB/ODS (Operational Data Store) which is on on-prem DB, some customer coming from REST API, also some data available in form of CSV or JSON formats. 
3] We get these the data then we use ADF to pull the data from these variety of data sources and we push to ADLS Gen2. 
4] Databricks kicked in and pull the data into Bronze layer, then after transformations goes to Silver layer and based on the business requirement we created our gold layer from which our DA/power BI team pulls the data from Gold Layer and create their dashboards.

End-to-End project structure -
1] Aim to create ETL, ELT pipelines. Here we are getting different kind of data from our source system like agent data, branch, policy, claims, customers & demographic data in raw format.
2] These data are coming from different source systems i.e. Rest API, on-prem database in CSV, parquet file format or it might be an SFTP server. Few files we are extracting from source and few files comes directly to ADLS Gen2. 
3] We are using Azure Data Factory for orchestration. Once raw data is in landing zone as in bronze layer, we mount ADLS Gen2 to databricks environment.
4] As soon as we mount to databricks environment, this raw data we are converting to delta tables, following medallion architecture in our project (Bronze, Silver, Gold). This delta tables are stored in silver layer. 
5] We do cleansing process in silver layer handling the nulls, duplicates, special characters or Jin chracters data handling in the silver layer.
6] Once the silver layer is ready, the we do business requirement aggregations i.e. highest sale, lowest sale like group By, min or max aggregations and storing into gold layer.
7] This gold layer data is been used by our downstream application systems and also DA, DS teams. DA team connect power BI to our gold layer for the visualizations and DS do some kind of ML algorithms.

Follow up questions:
What you did in this project?
-> My contributions is working on ADF and pyspark code in Azure databricks for cleaning purpose. 

Coming towards, while using COPY activity what was your sources?
->  1] On-prem, Oracle database
	2] REST API, SFTP servers where they will place their CSV or parquet files	

Migration from On-Premise to Azure SQL -
1] ADF can only fetch the data but it can't fetch structure
2] To move data as it is to cloud DB you can use data migration service

Self-hosted-integration runtime (SHIR) is the which we create which is needed for on-prem.
Azure gives auto-resolve-integration runtime (ARIR)

Tumbling window triger cater to one pipeline at a time.
Schedule Event Triger can trigger mutliple pipeline. 

Pipeline Failure occurs?
1] Tumbling Window (retry policy) its automatic 
2] Manually trigger (Monitor tab > Execution > Rerun from a failed activity)

CICD pipeline can run the code from GIT from one env to another.

Spark does not support DML operations unless we use Delta lake. Normally Spark cannot have PK,Fk constraints and we cannot replace DB with the spark tables by default does not support DML operations.

Questions:
===========

1] Team-size?
-> Total 7 DE including TL.

2] How many pipelines you have in your project?
-> 30-35 pipeline by me, in total there are 150-200 pipeline

3] Challenges in pipeline?
->	

3. i] How you did it?
-> Certain pipeline should work only on business days when there are no holidays in the company in-between Mon-Fri:
	1]Prepared a excel with all holidays from company's calender. 
	2] Before running pipeline we have a lookup activity, that lookup activity go to that file and check for a holiday..
	3] And we are using IF loop, if IF loop identifies that data and run the pipeline accordingly. 
	4] And in schedule trigger we just Monday-Friday.

4] Which SQL server you were using on-prem and how you fetch the data to cloud?
-> 	1] Oracle Database
	2] And we using SHIR in Azure and using COPY activity we were fetching the data to ADLS 
	
5] Why ARIR (Auto resolve Integration runtime) won't work for on-prem?
->	1] Its all due to Firewall reasons. ARIR is picking and calling it from random IP addresses.
	2] Probably your private network can not reached from that random IP addresses.
	3] Thats why SHIR will be connecting to on-prem DB server, because your database is expecting call from limited IP addresses.
	4] So that IP address gets whitelisted by the firewall of your server.

6] Where are you dumping data from your on-prem SQL server?
->  1] Via COPY activity we dump data in ADLS gen2 into Landing layer named for container in ADLS Gen2..
	2] Mount ADLS Gen2 to databricks to access rquired data.
	2] Then we copy those data into bronze layer using COPY activity inside ADLS Gen2.
	3] Data is getting saved into bronze layer as a delta table, creating table using Databricks. Creating bronze layer database as delta table.
	
7] How much GB of data you get per pipeline?
->	1] We deal around 1Tb of data on monthly basis. 
	2] We have 10 pipeline every pipeline is having 100GB.
	3] We are running 100 times a pipeline.

8] Spark Optimization?
->  1] Broadcast Hash Join
	2] Shuffle Hash Join (No. of buckets on both table needs to be same and yet sorted)
	2.i] We have lot of data scatterd accorss multiple countries.
	ii] To optimize we can partition the country column 
	3] Repartition to reduce skewness
	4] AQE (spark internally optimize job)
	
9] You got 1TB of data, you would optimize?
-> 	1] We dont get problem when we are using narrow transformations.
	2] Spark optimization kicks in when we dealing with wide transformations.
	3] Assume, we have lot wide transformations like Join. So we will focus on Join optimization i.e. repartition, bucketing, broadcast or SMJ.
	
10] How you would you move your code from one env to another?
->	
1] Development:
	Code is pushed to the dev branch.
	Azure DevOps triggers the CI pipeline to build and test.
	The CD pipeline deploys the ARM template and code to the Development environment.
2] Testing:
	Once the code passes development, it is merged into the test branch.
	CI pipeline validates, and the CD pipeline deploys the code and resources to the Test/QA environment.
	Integration and automated tests run.
3] Production:
	Once testing is successful, the code is merged into the main or prod branch.
	Approval gates are triggered for deployment to the Production environment.
	The CD pipeline deploys the code and infrastructure templates to production, with monitoring enabled for tracking.
	
11] Do you know global parameters in ADF?

12] How would you send an email notification when pipeline fails?
->	Using ADF via Alerts under Monitor tab / Azure Logic app

13] What makes tumbling windows trigger different from schedule window trigger?
->	1] System Variables
		1] We can set pipeline in intervals so it can create multiple window.
		2] When the pipeline invokes you will be access to start time/date and end time/date using system variables.
		3] There are two system variables:
			i] trigger().outputs.windowStartTime
			ii] trigger().outputs.windowEndTime
			Using above variables you can get values i.e. 5:14 AM - 6:14 AM window inside your pipeline execution
		4] Please note schedule trigger doesnt have such feature who will give such values.
		5] But still we can write current_timestamp function to achieve this in schedule trigger which will be hectic process.
		6] Using tumbling window you will access to start and end runtime using system variables.
	2] Backfilling the data
		1] You can only backfill the historic data via tumbling window.
		2] Using schedule window you can just get only present time/data runtime exactly match the trigger timing then only pipeline execute.
		3] Tumbling will work on future/past window.
	3] You can set dependencies between pipeline

Features of Tumbling window:
1] Max-concurrent in tumbling window executes number of pipeline at time parallely. It is helpful when you need to backfil the data.
2] Setting retry counts if failure.
3] When you want minimum job for each retry how much you job you want you can set in retry policy.
4] Tumbling is 1-1. If you have multiple pipelines you end up creating multiple tumbling window. 
	
14] Daily activities of data engineer?
->	1] Dealing/managing pipelines
	2] Deliver pipelines according to business requirement
	3] Monitoring/managing existing pipelines, fixing breaking/failure pipelines.
	4] Writing big data logic in pyspark code based on requirements.

15] how you are gettind data into your landing zone?
-> We are using ADF for orchestration and using COPY ACTIVITY to fetch data from source to destination.

16] Why you need delta tables?
-> 
1] It supports ACID compliance and its helpful in big data analytics. Effectivity doing DML operations i.e. insert, update, delete.
2] It suppports time travel features, version features.

17] How do you manage schema changes in your data pipeline?
-> Dynamic Schema Handling in pipeline:
i] ADF
In a Mapping Data Flow, you can enable "Allow Schema Drift" to ensure your pipeline adapts to changes such as a new column being added to a CSV file.
ii] pyspark
Use flexible data formats like Delta Lake, which supports schema evolution and versioning.
Use mergeSchema during read operations to merge new schema changes into the existing dataset dynamically.
"df = spark.read.format("delta").option("mergeSchema", "true").load("path_to_delta_table")"
"df.write.option("mergeSchema", "true").format("delta").save("path_to_table")
"df.write.option("mergeSchema", "true").format("delta").save("path_to_table")"

Follow up questions:
Do you really need to delta tables in your project?
-> 1] To check whatever the new changes happened in delta table 

Pipeline parameters are runtime paramaters while executing pipeline.
Global parameters available on pipeline level.

1. How Many Pipelines You Manage?
If you handle 7-10GB of data daily, the number of pipelines would depend on how the data is structured, the sources, and the processing requirements. For example, if data comes from different sources like transactional databases, APIs, or batch files, you could mention handling around 5-10 pipelines daily.

Explain that each pipeline may represent different stages like data ingestion, transformation, validation, and loading into a data warehouse or data lake, with some designed for daily processing and others possibly for incremental or hourly loads.

2. Total Monthly Data Across the Team
If you individually handle 7-10GB of data per day, it would mean youâ€™re processing approximately 210-300GB of data per month (7-10GB * 30 days).

With 7 Data Engineers working at a similar scale, the team might manage 1.47TB to 2.1TB of data monthly.

3. Realistic Answer
You can say something like:
"In my current role, I work with a team of seven Data Engineers, and we collectively manage around 1.5 to 2TB of data monthly. Personally, I handle 7-10GB of data daily, which involves managing 5-10 pipelines that cover the full data lifecycle from ingestion to processing in our Azure Data Lake. Each pipeline may differ based on the source and processing requirements, ensuring our data remains structured and available for business needs."

This response demonstrates awareness of the team effort, data volume, and complexity in managing multiple pipelines.

Initial follow up questions
============================
1. Can you describe the types of pipelines you manage?
Tip: Mention specific types of pipelines, such as batch processing, streaming, or real-time ingestion, depending on your experience. Describe steps like ingestion, transformation, data validation, and loading into Azure Data Lake or a warehouse.
Example Answer: "I primarily work with batch pipelines to process daily data dumps, which include ingestion from sources like SQL databases and cloud storage, transformation using PySpark, and validation. We also have incremental pipelines for updates, where only modified data gets processed to save costs and improve performance."

2. How do you ensure data quality in your pipelines?
Tip: Discuss data quality checks like schema validation, data profiling, handling missing values, and using tools or custom scripts for validation.
Example Answer: "We implement data validation steps to check schema compatibility and use regex or range checks to ensure data consistency. We also use Azure Data Factory for setting up validation steps at various stages and Databricks to handle transformations and cleanse data."

3. What Azure services do you use in your pipelines?
Tip: Mention services like Azure Data Factory, Azure Databricks, Azure Data Lake Storage (ADLS), Azure Synapse Analytics, and Azure Monitor for pipeline monitoring and optimization.
Example Answer: "Our pipelines are built using Azure Data Factory for orchestration, Azure Databricks for heavy data transformations in PySpark, and Azure Data Lake Storage for storing raw and processed data. We also leverage Azure Synapse for analysis and reporting, and Azure Monitor for tracking pipeline health."

4. What challenges have you faced while managing such large data volumes?
Tip: Share challenges like performance tuning, handling schema evolution, or troubleshooting failed jobs.
Example Answer: "One challenge was optimizing the performance of PySpark jobs on Databricks as the data volume grew. We addressed this by partitioning the data better and optimizing joins and aggregations, which improved processing time by around 30%."

5. How do you optimize pipelines for performance and cost efficiency?
Tip: Discuss cost-saving strategies like using cluster autoscaling in Databricks, managing data storage tiers in ADLS, and processing only incremental changes when possible.
Example Answer: "We optimize pipelines by using Spark caching in Databricks for frequently accessed data, adjusting cluster sizes based on job complexity, and implementing data partitioning and pruning to reduce unnecessary data scans. We also leverage Azure Data Lake's tiered storage to manage costs."

6. How do you monitor and handle pipeline failures?
Tip: Talk about using Azure Monitor, custom alerts, logging in Azure Databricks, and retry policies for failed jobs.
Example Answer: "We use Azure Monitor to set up alerts for job failures and performance degradation, and we apply logging within Databricks notebooks to track the steps in each pipeline. If a pipeline fails, we analyze logs, identify the root cause, and use retry policies to reprocess failed data."

7. What is your approach to handling data schema changes in a pipeline?
Tip: Describe methods like schema versioning, using DataFrames in PySpark, or implementing schema drift in Azure Data Factory.
Example Answer: "We handle schema changes by defining schema checks at the start of each pipeline. In cases of schema drift, we use the schema drift features in Azure Data Factory and have created scripts in PySpark to accommodate new fields or types, ensuring downstream systems aren't impacted."

8. How do you manage incremental data loads?
Tip: Explain how you identify changes and load only modified data using watermarking or timestamps.
Example Answer: "We use a watermarking technique that relies on timestamps to capture only new or modified records. This method helps us handle large datasets efficiently, especially when loading data into our data lake or warehouse, as it avoids full data reloads and reduces processing costs."

9. How does your team collaborate on such large projects?
Tip: Mention task distribution, code reviews, version control, and using tools like Azure DevOps.
Example Answer: "We divide tasks based on expertise, conduct regular code reviews for best practices, and use Azure DevOps for version control. This setup enables smooth collaboration and consistency across the team, especially as we manage high-volume data processing."
  
10. How do you document your data pipelines?
Tip: Highlight using wikis, markdown files in version control, and automated documentation tools in Azure.
Example Answer: "We maintain documentation on Azure DevOps, including pipeline steps, configurations, and known issues, ensuring all team members have access to up-to-date information. Additionally, we generate automated pipeline diagrams to make it easy to understand dependencies and workflows."
