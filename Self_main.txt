Inteview is where we need to kill the time
https://www.youtube.com/watch?v=jQhc1NwdOOg&list=WL&index=1&t=718s&ab_channel=AzurelibAcademy

https://www.linkedin.com/posts/shubhamwadekar_data-warehousing-interview-questions-%F0%9D%97%A3%F0%9D%97%9B%F0%9D%97%94%F0%9D%97%A6%F0%9D%97%98-activity-7247250823871705095-J9WD

Self-Introduction
==================
My name is Anuj Gogate. I am from Pune. Currently working as Data Engineer in Teradata having total 4+ years of IT experience. I am proficient in ACID data engineer using ADF, Azure Databricks. I have command in writing Pyspark, Python and SQL. This is brief of myself.

My data engineer roles is to transform the raw data into suitaable for DA, DS, ML purpose. We are using Azure in our project 

Retail-hub
===========

End-to-End project structure -
1] Its a Retail Domain project. This is a project of Data lake house project where we follow medallion architecture of bronze, silver and gold layers.
2] The Injection of the data happens from the upstream system we have a data source like SQL DB/ODS (Operational Data Store) which is on on-prem DB, some customer coming from REST API, also some data available in form of CSV or JSON formats. 
3] We get these the data then we use ADF to pull the data from these variety of data sources and we push to ADLS Gen2. 
4] Databricks kicked in and pull the data into Bronze layer, then after transformations goes to Silver layer and based on the business requirement we created our gold layer from which our DA/power BI team pulls the data from Gold Layer and create their dashboards.

End-to-End project structure -
1] Aim to create ETL, ELT pipelines. Here we are getting different kind of data from our source system like agent data, branch, policy, claims, customers & demographic data in raw format.
2] These data are coming from different source systems i.e. Rest API, on-prem database in CSV, parquet file format or it might be an SFTP server. Few files we are extracting from source and few files comes directly to ADLS Gen2. 
3] We are using Azure Data Factory for orchestration. Once raw data is in landing zone as in bronze layer, we mount ADLS Gen2 to databricks environment.
4] As soon as we mount to databricks environment, this raw data we are converting to delta tables, following medallion architecture in our project (Bronze, Silver, Gold). This delta tables are stored in silver layer. 
5] We do cleansing process in silver layer handling the nulls, duplicates, special characters or Jin chracters data handling in the silver layer.
6] Once the silver layer is ready, the we do business requirement aggregations i.e. highest sale, lowest sale like group By, min or max aggregations and storing into gold layer.
7] This gold layer data is been used by our downstream application systems and also DA, DS teams. DA team connect power BI to our gold layer for the visualizations and DS do some kind of ML algorithms.

Follow up questions:
What you did in this project?
-> My contributions is working on ADF and pyspark code in Azure databricks for cleaning purpose. 

Coming towards, while using COPY activity what was your sources?
->  1] On-prem, Oracle database
	2] REST API, SFTP servers where they will place their CSV or parquet files	

Migration from On-Premise to Azure SQL -
1] ADF can only fetch the data but it can't fetch structure
2] To move data as it is to cloud DB you can use data migration service

Self-hosted-integration runtime (SHIR) is the which we create which is needed for on-prem.
Azure gives auto-resolve-integration runtime (ARIR)

Tumbling window triger cater to one pipeline at a time.
Schedule Event Triger can trigger mutliple pipeline. 

Pipeline Failure occurs?
1] Tumbling Window (retry policy) its automatic 
2] Manually trigger (Monitor tab > Execution > Rerun from a failed activity)

CICD pipeline can run the code from GIT from one env to another.

Spark does not support DML operations unless we use Delta lake. Normally Spark cannot have PK,Fk constraints and we cannot replace DB with the spark tables by default does not support DML operations.

Questions:
===========

1] Team-size?
-> Total 7 DE including TL.

2] How many pipelines you have in your project?
-> 30-35 pipeline by me, in total there are 150-200 pipeline

3] Challenges in pipeline?
->	

3. i] How you did it?
-> Certain pipeline should work only on business days when there are no holidays in the company in-between Mon-Fri:
	1]Prepared a excel with all holidays from company's calender. 
	2] Before running pipeline we have a lookup activity, that lookup activity go to that file and check for a holiday..
	3] And we are using IF loop, if IF loop identifies that data and run the pipeline accordingly. 
	4] And in schedule trigger we just Monday-Friday.

4] Which SQL server you were using on-prem and how you fetch the data to cloud?
-> 	1] Oracle Database
	2] And we using SHIR in Azure and using COPY activity we were fetching the data to ADLS 
	
5] Why ARIR (Auto resolve Integration runtime) won't work for on-prem?
->	1] Its all due to Firewall reasons. ARIR is picking and calling it from random IP addresses.
	2] Probably your private network can not reached from that random IP addresses.
	3] Thats why SHIR will be connecting to on-prem DB server, because your database is expecting call from limited IP addresses.
	4] So that IP address gets whitelisted by the firewall of your server.

6] Where are you dumping data from your on-prem SQL server?
->  1] Via COPY activity we dump data in ADLS gen2 into Landing layer named for container in ADLS Gen2..
	2] Mount ADLS Gen2 to databricks to access rquired data.
	2] Then we copy those data into bronze layer using COPY activity inside ADLS Gen2.
	3] Data is getting saved into bronze layer as a delta table, creating table using Databricks. Creating bronze layer database as delta table.
	
7] How much GB of data you get per pipeline?
->	1] We deal around 1Tb of data on monthly basis. 
	2] We have 10 pipeline every pipeline is having 100GB.
	3] We are running 100 times a pipeline.

8] Spark Optimization?
->  1] Broadcast Hash Join
	2] Shuffle Hash Join (No. of buckets on both table needs to be same and yet sorted)
	2.i] We have lot of data scatterd accorss multiple countries.
	ii] To optimize we can partition the country column 
	3] Repartition to reduce skewness
	4] AQE (spark internally optimize job)
	
9] You got 1TB of data, you would optimize?
-> 	1] We dont get problem when we are using narrow transformations.
	2] Spark optimization kicks in when we dealing with wide transformations.
	3] Assume, we have lot wide transformations like Join. So we will focus on Join optimization i.e. repartition, bucketing, broadcast or SMJ.
	
10] How you would you move your code from one env to another?
->	
1] Development:
	Code is pushed to the dev branch.
	Azure DevOps triggers the CI pipeline to build and test.
	The CD pipeline deploys the ARM template and code to the Development environment.
2] Testing:
	Once the code passes development, it is merged into the test branch.
	CI pipeline validates, and the CD pipeline deploys the code and resources to the Test/QA environment.
	Integration and automated tests run.
3] Production:
	Once testing is successful, the code is merged into the main or prod branch.
	Approval gates are triggered for deployment to the Production environment.
	The CD pipeline deploys the code and infrastructure templates to production, with monitoring enabled for tracking.
	
11] Do you know global parameters in ADF?

12] How would you send an email notification when pipeline fails?
->	Using ADF via Alerts under Monitor tab / Azure Logic app

13] What makes tumbling windows trigger different from schedule window trigger?
->	1] System Variables
		1] We can set pipeline in intervals so it can create multiple window.
		2] When the pipeline invokes you will be access to start time/date and end time/date using system variables.
		3] There are two system variables:
			i] trigger().outputs.windowStartTime
			ii] trigger().outputs.windowEndTime
			Using above variables you can get values i.e. 5:14 AM - 6:14 AM window inside your pipeline execution
		4] Please note schedule trigger doesnt have such feature who will give such values.
		5] But still we can write current_timestamp function to achieve this in schedule trigger which will be hectic process.
		6] Using tumbling window you will access to start and end runtime using system variables.
	2] Backfilling the data
		1] You can only backfill the historic data via tumbling window.
		2] Using schedule window you can just get only present time/data runtime exactly match the trigger timing then only pipeline execute.
		3] Tumbling will work on future/past window.
	3] You can set dependencies between pipeline

Features of Tumbling window:
1] Max-concurrent in tumbling window executes number of pipeline at time parallely. It is helpful when you need to backfil the data.
2] Setting retry counts if failure.
3] When you want minimum job for each retry how much you job you want you can set in retry policy.
4] Tumbling is 1-1. If you have multiple pipelines you end up creating multiple tumbling window. 
	
14] Daily activities of data engineer?
->	1] Dealing/managing pipelines
	2] Deliver pipelines according to business requirement
	3] Monitoring/managing existing pipelines, fixing breaking/failure pipelines.
	4] Writing big data logic in pyspark code based on requirements.

15] how you are gettind data into your landing zone?
-> We are using ADF for orchestration and using COPY ACTIVITY to fetch data from source to destination.

16] Why you need delta tables?
-> 
1] It supports ACID compliance and its helpful in big data analytics. Effectivity doing DML operations i.e. insert, update, delete.
2] It suppports time travel features, version features.

Follow up questions:
Do you really need to delta tables in your project?
-> 1] To check whatever the new changes happened in delta table 

Pipeline parameters are runtime paramaters while executing pipeline.
Global parameters available on pipeline level.